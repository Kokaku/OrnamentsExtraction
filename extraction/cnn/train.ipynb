{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "from scipy import stats\n",
    "from math import sqrt, inf, isnan\n",
    "import hashlib\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import Process\n",
    "import time\n",
    "from subprocess import Popen, PIPE\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import scipy.io\n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../utilities\")\n",
    "sys.path.append(\"../regionProposal\")\n",
    "from imageHandler import getImagePath, readImage, squareResizeImage, getCropImage, parseOrnament\n",
    "from EvaluationHelper import findBestIou, getProposalsIou\n",
    "from log import log_progress\n",
    "from regionProposal import processSelectiveSearch\n",
    "\n",
    "tf.python.control_flow_ops = tf\n",
    "multManager = multiprocessing.Manager()\n",
    "\n",
    "outputFolder = '/scratch/fcjunker/output/'\n",
    "jsonsExtractionPath = '/mnt/cluster-nas/florian/extractionResults/'\n",
    "imagesPath = '/mnt/Ornaments_IMG/'\n",
    "\n",
    "currentFolder = os.path.dirname(os.path.realpath('test.py'))\n",
    "script_dirname = currentFolder+'/../regionProposal/matlabEdgeBoxes/'\n",
    "annotatedPagesJsonPath = currentFolder+'/../../data/annotatedPages.json'\n",
    "annotatedPagesSortedJsonPath = currentFolder+'/../../data/annotatedPagesSorted.json'\n",
    "\n",
    "sizes = {\n",
    "    'testSize': 3000,\n",
    "    'trainSize': 4000\n",
    "    }\n",
    "\n",
    "numProcess = 24\n",
    "\n",
    "np.set_printoptions(threshold='nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gpuConfigs import configureGpu\n",
    "\"\"\"\n",
    "    Configure GPU usage for Keras with Tensorflow.\n",
    "\"\"\"\n",
    "configureGpu(gpuId='0', mode='limit', limit=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createModel(verbose=True):\n",
    "    \"\"\"\n",
    "    Create a Keras model for the CNN\n",
    "    Its input shape is (1, 7, 7, 512) that correspond to the output of VGG16 given an image of 256x256\n",
    "    Its output is a real value between 0 and 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, 7, 7, 512)))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "        \n",
    "    return model\n",
    "\n",
    "def trainModel(model, trainingSet, testingSet, iouThreshold=0.5, maxNegExamplePerPage=2, verbose=2, nb_epoch=15):\n",
    "    \"\"\"\n",
    "    Train the given model with the given training set.\n",
    "    \"\"\"\n",
    "    \n",
    "    trainingGenerator = generateClassifierSample(iouThreshold, maxNegExamplePerPage, trainingSet)\n",
    "    testingGenerator = generateClassifierSample(iouThreshold, maxNegExamplePerPage, testingSet)\n",
    "\n",
    "    model.fit_generator(\n",
    "            trainingGenerator,\n",
    "            samples_per_epoch = countGeneratedProposals(trainingSet),\n",
    "            validation_data = testingGenerator,\n",
    "            nb_val_samples = countGeneratedProposals(testingSet),\n",
    "            max_q_size=3, nb_epoch=nb_epoch, verbose=verbose)\n",
    "\n",
    "def generateClassifierSample(iouThreshold, numNegProposalsPerPage, annotatedPages):\n",
    "    \"\"\"\n",
    "    A generator that yield a tuple (input, output) for the ornament classifier.\n",
    "    Output at most (in average) 'numNegProposalsPerPage' negative examples per page and all positive examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputData = []\n",
    "    outputData = []\n",
    "    batchSize = 128\n",
    "    \n",
    "    while True:\n",
    "        adjustNumNegProposals = 0\n",
    "        for key in annotatedPages:\n",
    "            perm = np.random.permutation(len(annotatedPages[key]))\n",
    "            for index in range(len(annotatedPages['annotatedPagesWithOrnament'])):\n",
    "                annotatedPage = annotatedPages[key][perm[index]]\n",
    "                pagePath = getImagePath(imagesPath+\"bookm-\"+annotatedPage['bookId']+\"/\"+annotatedPage['pageId'])\n",
    "                fullImage = readImage(pagePath)\n",
    "\n",
    "                ornaments = list(map(parseOrnament, annotatedPage['ornaments']))\n",
    "                numPosProposals = np.sum(np.array(annotatedPage['proposalsScores']) > iouThreshold)\n",
    "                numNegProposals = numNegProposalsPerPage+adjustNumNegProposals\n",
    "                extractedOrnaments = getRandomProposals(annotatedPage, threshold=iouThreshold, cut=numNegProposals)\n",
    "                adjustNumNegProposals = numNegProposals - len(extractedOrnaments) + numPosProposals\n",
    "\n",
    "                for extractedOrnamentStr in extractedOrnaments:\n",
    "                    extractedOrnament = parseOrnament(extractedOrnamentStr)\n",
    "                    im = squareResizeImage(getCropImage(extractedOrnament, fullImage), 244)\n",
    "\n",
    "                    inputData.append(extractFeatures(im))\n",
    "                    outputData.append(findBestIou(ornaments, extractedOrnamentStr) > iouThreshold)\n",
    "\n",
    "                    if len(inputData) >= batchSize:\n",
    "                        yield (np.asarray(inputData), np.asarray(outputData))\n",
    "                        inputData = []\n",
    "                        outputData = []\n",
    "\n",
    "        while adjustNumNegProposals > 0:\n",
    "            adjustNumNegProposals -= 1\n",
    "            inputData.append(extractFeatures(im))\n",
    "            outputData.append(findBestIou(ornaments, extractedOrnamentStr))\n",
    "\n",
    "\n",
    "        yield (np.asarray(inputData), np.asarray(outputData))\n",
    "        \n",
    "def getRandomProposals(annotatedPage, threshold=0.5, cut=2):\n",
    "    \"\"\"\n",
    "    Shuffle proposals and output at most'cut' negatives examples per page and all positive examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    indices = np.array(annotatedPage['proposalsScores']) > threshold\n",
    "    arr = np.array(annotatedPage['proposals'])\n",
    "\n",
    "    neg = arr[indices == False]\n",
    "    p = np.random.permutation(len(neg))\n",
    "    res = np.concatenate((neg[p][:cut], arr[indices]), axis=0)\n",
    "    random.shuffle(res)\n",
    "    return res\n",
    "        \n",
    "def countGeneratedProposals(dataset):\n",
    "    \"\"\"\n",
    "    Count the number of proposals in one pass (typically one epoch).\n",
    "    \"\"\"\n",
    "    \n",
    "    threshold = 0.5\n",
    "    count = 0\n",
    "    for annotatedPage in dataset['annotatedPagesWithOrnament']:\n",
    "        val = np.sum(np.array(annotatedPage['proposalsScores']) > threshold)\n",
    "        count += val\n",
    "\n",
    "    count += 4*len(dataset['annotatedPagesWithOrnament'])\n",
    "    return count\n",
    "    \n",
    "def getProposals(annotatedPage, proposalsFolderPath):\n",
    "    \"\"\"\n",
    "    Read proposals from disk.\n",
    "    Note it assume that proposals from median filter and selective search are written on disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Read Median filter proposals\n",
    "    if not os.path.isdir(\"{0}median/\".format(proposalsFolderPath)):\n",
    "        print(\"Please write proposals from median filter in the following folder before proceding:\")\n",
    "        print(\"{0}median/\".format(proposalsFolderPath))\n",
    "        return []\n",
    "    \n",
    "    proposalMedianJsonPath = \"{0}median/{1}.json\".format(proposalsFolderPath, annotatedPage['bookId'])\n",
    "    proposalMedianJson = open(proposalMedianJsonPath).read()\n",
    "    pages = json.loads(proposalMedianJson)['pages']\n",
    "    \n",
    "    medianProposals = []\n",
    "    for currentPage in pages:\n",
    "        if annotatedPage['pageId'] == currentPage['id']:\n",
    "            medianProposals = currentPage['segments']\n",
    "    \n",
    "    \n",
    "    #Read selective search proposals\n",
    "    proposalsSelectiveSearchFolderPath = '{}selectiveSearch/'.format(proposalsFolderPath)\n",
    "    proposalSelSearchJsonPath = '{0}{1}-{2}.json'.format(proposalsFolderPath,\n",
    "                                                         annotatedPage['bookId'], annotatedPage['pageId'])\n",
    "    proposalSelSearchJson = open(proposalSelSearchJsonPath).read()\n",
    "    \n",
    "    return json.loads(proposalSelSearchJson)['candidates'] + medianProposals\n",
    "\n",
    "def genProposals(annotatedPages, proposalsFolderPath):\n",
    "    \"\"\"\n",
    "    Generate proposals with selective search.\n",
    "    \"\"\"\n",
    "    \n",
    "    pagesToProcessJsonPath = '{}genProposals.json'.format(proposalsFolderPath)\n",
    "    proposalsSelectiveSearchFolderPath = '{}selectiveSearch/'.format(proposalsFolderPath)\n",
    "\n",
    "    pagesToProcess = []\n",
    "    for key in annotatedPages:\n",
    "        for annotatedPage in annotatedPages[key]:\n",
    "            pagesToProcess.append({\n",
    "                'bookId': annotatedPage['bookId'],\n",
    "                'pageId': annotatedPage['pageId']\n",
    "            })\n",
    "\n",
    "    jsonFile = open(pagesToProcessJsonPath, \"w\")\n",
    "    jsonFile.write(json.dumps({'annotatedPages': pagesToProcess}, indent=4, sort_keys=True))\n",
    "    jsonFile.close()\n",
    "\n",
    "    print(\"Generating proposals\")\n",
    "    processSelectiveSearch(pagesToProcessJsonPath, proposalsFolderPath, resize=500, scale=500, sigma=0.9,\n",
    "                           minSize=100, numProcess=numProcess)\n",
    "    print(\"Proposals generation finished\")\n",
    "\n",
    "def extractFeaturesVGG16(modelVgg16, x):\n",
    "    if len(x.shape) == 2:\n",
    "        x = cv2.cvtColor(x,cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    if len(x.shape) != 3:\n",
    "        print('Error, image with shape: ', x.shape)\n",
    "    \n",
    "    x = x.astype('float')\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return modelVgg16.predict(x)\n",
    "\n",
    "def preparedata(size, frontData = True):\n",
    "    dataset = {}\n",
    "    numExamplePerPage = {}\n",
    "    numExample = {}\n",
    "    numPosExamples = 0\n",
    "    for key in annotatedPages:\n",
    "        if frontData:\n",
    "            dataset[key] = annotatedPages[key][:size[key]]\n",
    "        else:\n",
    "            dataset[key] = annotatedPages[key][-size[key]:]\n",
    "\n",
    "        numExamplePerPage[key] = np.asarray(list(map(lambda x: len(x['proposals']), dataset[key])))\n",
    "        numExample[key] = np.sum(numExamplePerPage[key])\n",
    "\n",
    "    return dataset, numExamplePerPage, numExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Create a keras model to extract VGG 16 features from 244x244x3 images\n",
    "\"\"\"\n",
    "modelVgg16 = VGG16(weights='imagenet', include_top=False)\n",
    "extractFeatures = partial(extractFeaturesVGG16, modelVgg16)\n",
    "extractFeatures(np.ones((244, 244, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Read pages annotation from disk\n",
    "\"\"\"\n",
    "annotatedPagesJson = open(annotatedPagesSortedJsonPath).read()\n",
    "annotatedPages = json.loads(annotatedPagesJson)['annotatedPages']\n",
    "numPagesWithOrnament = len(annotatedPages['annotatedPagesWithOrnament'])\n",
    "numPagesWithoutOrnament = len(annotatedPages['annotatedPagesWithoutOrnament'])\n",
    "numPages = numPagesWithOrnament+numPagesWithoutOrnament\n",
    "\n",
    "testSize = {\n",
    "    'annotatedPagesWithOrnament': round(numPagesWithOrnament/numPages * sizes['testSize']),\n",
    "    'annotatedPagesWithoutOrnament': round(numPagesWithoutOrnament/numPages * sizes['testSize'])\n",
    "}\n",
    "trainSize = {\n",
    "    'annotatedPagesWithOrnament': round(numPagesWithOrnament/numPages * sizes['trainSize']),\n",
    "    'annotatedPagesWithoutOrnament': round(numPagesWithoutOrnament/numPages * sizes['trainSize'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Generate, write an read proposals for each page\n",
    "\"\"\"\n",
    "proposalsFolderPath = '{}proposals/'.format(outputFolder)\n",
    "genProposals(annotatedPages, proposalsFolderPath)\n",
    "\n",
    "print('Read regions proposed')\n",
    "for key in annotatedPages:\n",
    "    for annotatedPage in log_progress(annotatedPages[key], 1):\n",
    "        annotatedPage['proposals'] = getProposals(annotatedPage, proposalsFolderPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Rank each proposals (IoU)\n",
    "\"\"\"\n",
    "workers = Pool(processes=numProcess, maxtasksperchild=100)\n",
    "for key in annotatedPages:\n",
    "    dataset = annotatedPages[key]\n",
    "    datsetIndex = 0\n",
    "    for proposalsIou in log_progress(enumerate(workers.imap(getProposalsIou, dataset), 1), 1, len(dataset)):\n",
    "        dataset[datsetIndex]['proposalsScores'] = proposalsIou[1]\n",
    "        datsetIndex += 1\n",
    "\n",
    "workers.close()\n",
    "workers.terminate()\n",
    "workers.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Train the classifier model\n",
    "\"\"\"\n",
    "testingSet, numTestingExamplePerPage, numTestingExample = preparedata(testSize, frontData=False)\n",
    "trainingSet, numTrainingExamplePerPage, numTrainingExample = preparedata(trainSize)\n",
    "\n",
    "specializedNetwork = createModel()\n",
    "trainModel(specializedNetwork, trainingSet, testingSet)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
